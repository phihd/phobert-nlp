{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "introductory-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True to skip training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "black-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-wrist",
   "metadata": {},
   "source": [
    "# 1. Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-commission",
   "metadata": {},
   "source": [
    "https://github.com/datquocnguyen/VnDT#data-split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-berry",
   "metadata": {},
   "source": [
    "https://github.com/datquocnguyen/VnDT/blob/master/VnDT-paper-CameraReadyVersion.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-manner",
   "metadata": {},
   "source": [
    "## 1.1 Read the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "temporal-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_1(file_name):\n",
    "    with open(file_name, encoding='utf8') as f:\n",
    "        # line = ['Np Proper noun','Nc Classifier noun',...]\n",
    "        lines = re.split('\\n',f.read())\n",
    "    \n",
    "    tags = []\n",
    "    tag_dict = {}\n",
    "    for line in lines:\n",
    "        tmp = line.split(' ',1)\n",
    "        tags.append(tmp[0])\n",
    "        tag_dict[tmp[0]] = tmp[1]\n",
    "        \n",
    "    return tags, tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-sugar",
   "metadata": {},
   "source": [
    "## 1.2 Read train - dev - test corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "violent-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_2(file_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name - string\n",
    "        a path to a file with an annotated corpus\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    words - a list of lists of words\n",
    "    tags - a list of lists of tags\n",
    "        For example, the first sentence in a file is word1-tag1, word2-tag2 \n",
    "        and the next sentence is word3_/_tag3. Then you should get:\n",
    "        words = [['word1','word2'],['word3']]\n",
    "        tags = [['tag1','tag2'],['tag3']]\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_name, sep='\\t|\\n', names = ['idx','word','c3','c4','tag','c6','c7','c8','c9','c10'])\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    sentence_words = []\n",
    "    sentence_tags = []\n",
    "    prev_idx = 0\n",
    "    for _,row in df.iterrows():\n",
    "        if row['idx'] != prev_idx+1:\n",
    "            words.append(sentence_words)\n",
    "            tags.append(sentence_tags)\n",
    "            sentence_words = []\n",
    "            sentence_tags = []\n",
    "            \n",
    "        sentence_words.append(row['word'])\n",
    "        sentence_tags.append(row['tag'])\n",
    "        prev_idx = row['idx']\n",
    "            \n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-haiti",
   "metadata": {},
   "source": [
    "## 1.3 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fa0f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/m/home/home1/12/dangp1/unix/POS_NER'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "streaming-credits",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/12/dangp1/unix/.conda/envs/concac/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Lần', 'này', 'Thọ', 'xưng', 'rõ', 'danh_tính', 'với', 'bà', '(', 'vì', 'là', 'chỗ', 'thân_tình', '(', 'và', 'tâm_sự', 'chuyện', 'gia_đình', 'đang', 'có', 'rắc_rối', '.'], ['Thế', 'nhưng', 'tính', 'nhân_văn', 'này', 'chưa', 'đủ', 'sức', 'thuyết_phục', 'trong', 'cộng_đồng', '.'], ['Khóc', 'mãi', ',', 'khóc', 'suốt', 'đêm', 'cũng', 'không', 'hiểu', 'được', 'điều', 'gì', 'sẽ', 'xảy', 'đến', 'ngày_mai', '...'], ['Làm', 'gì', 'để', 'DN', 'và', 'hàng', 'VN', 'sang', 'CPC', 'nhiều', 'hơn', '?'], ['Khi', 'thám_tử', 'Tuấn', 'về', 'đến', 'Hà_Nội', ',', 'hoàn_thành', 'vai', 'sinh_viên', 'Hải', 'thì', 'Công_ty', 'điều_tra', '&', 'bảo_vệ', 'V', 'cũng', 'hoàn_tất', 'hợp_đồng', 'với', 'chị', 'Hằng', '.']]\n",
      "[['N', 'P', 'Np', 'V', 'A', 'N', 'E', 'N', 'CH', 'E', 'V', 'N', 'A', 'CH', 'Cc', 'V', 'N', 'N', 'R', 'V', 'A', 'CH'], ['T', 'C', 'N', 'A', 'P', 'R', 'A', 'N', 'V', 'E', 'N', 'CH'], ['V', 'R', 'CH', 'V', 'A', 'N', 'R', 'R', 'V', 'R', 'N', 'P', 'R', 'V', 'R', 'N', 'CH'], ['V', 'P', 'E', 'Ny', 'Cc', 'N', 'Ny', 'V', 'Ny', 'A', 'R', 'CH'], ['N', 'N', 'Np', 'V', 'E', 'Np', 'CH', 'V', 'N', 'N', 'Np', 'C', 'N', 'V', 'Ny', 'V', 'Ny', 'R', 'V', 'N', 'E', 'Nc', 'Np', 'CH']]\n"
     ]
    }
   ],
   "source": [
    "tags_vocab_path = cwd+'/data/tags_vocab.txt'\n",
    "train_path = cwd+'/data/train.txt'\n",
    "test_path = cwd+'/data/test.txt'\n",
    "\n",
    "tags_vocab, tags_vocab_dict = read_1(tags_vocab_path)\n",
    "train_words, train_tags = read_2(train_path)\n",
    "test_words, test_tags = read_2(test_path)\n",
    "\n",
    "print(train_words[:5])\n",
    "print(train_tags[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-recycling",
   "metadata": {},
   "source": [
    "## 1.4 Enumerate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "municipal-jaguar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tag2num(tags_vocab):\n",
    "    return dict(zip( tags_vocab, range(1, len(tags_vocab)+1) ))\n",
    "\n",
    "tag2num = tag2num(tags_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "median-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Enumerate_tags(sentence_tags, tag2num):\n",
    "    converted_tags = [ list(map(lambda x: tag2num[x], sentence_tag)) for sentence_tag in sentence_tags ]\n",
    "    return converted_tags\n",
    "\n",
    "enumerated_train_tags = Enumerate_tags(train_tags, tag2num)\n",
    "enumerated_test_tags = Enumerate_tags(test_tags, tag2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "african-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-capacity",
   "metadata": {},
   "source": [
    "# 2. Study the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-recruitment",
   "metadata": {},
   "source": [
    "# 3. Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ahead-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(Dataset):\n",
    "    def __init__(self, words, tags, tags_vocab, tokenizer, max_len, pad_idx):\n",
    "        self.data = words\n",
    "        self.labels = tags\n",
    "        self.tags_vocab = tags_vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.len = len(self.data)\n",
    "        self.max_len = max_len\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.data[index],\n",
    "            None,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            padding = 'max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        \n",
    "        labels = self.labels[index] \n",
    "        labels += [self.pad_idx]*(self.max_len-len(labels))      # pad to the right\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distributed-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "running-champion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fa825202424e15b7f75ad8ff43c73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/874k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef20532851b9492781b2619c52d2ec58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "training_set = Encode(train_words, enumerated_train_tags, tags_vocab, tokenizer, MAX_LEN, PAD_IDX)\n",
    "testing_set = Encode(test_words, enumerated_test_tags, tags_vocab, tokenizer, MAX_LEN, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "addressed-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-mayor",
   "metadata": {},
   "source": [
    "# 4. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amino-values",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/VinAIResearch/PhoBERT\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ordinary-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "focused-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/bentrevett/pytorch-pos-tagging/blob/master/2_transformer.ipynb\n",
    "class PhoBERTPoSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super(PhoBERTPoSTagger, self).__init__()\n",
    "        self.bert = bert\n",
    "        for params in self.bert.parameters():\n",
    "            params.requires_grad =  False\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.fc1 = nn.Linear(embedding_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1024 )\n",
    "\n",
    "        self.fc = nn.Linear(1024, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, ids):\n",
    "  \n",
    "        # ids = (batch_size, max_len)\n",
    "    \n",
    "        outputs = self.bert(\n",
    "            input_ids=ids,\n",
    "        )\n",
    "        \n",
    "        # outputs = BERT return\n",
    "    \n",
    "        outputs = outputs[0]\n",
    "        \n",
    "        # outputs = (batch_size, max_len, hidden_size)\n",
    "        \n",
    "        outputs = self.fc1(self.dropout(outputs))\n",
    "        outputs = self.fc2(self.dropout(outputs))\n",
    "        # outputs = (batch_size, max_len, output_dim)\n",
    "        outputs = self.fc(self.dropout(outputs))\n",
    "        outputs = outputs.permute(1,0,2)\n",
    "        \n",
    "        # outputs = (max_len, batch_size, output_dim)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unexpected-waters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dim = len(tags_vocab)+1\n",
    "dropout = 0.25\n",
    "\n",
    "model = PhoBERTPoSTagger(phobert,\n",
    "                      output_dim, \n",
    "                      dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-processing",
   "metadata": {},
   "source": [
    "# 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "composite-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "driven-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "second-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels, pad_idx):\n",
    "    max_pred_idx = preds.argmax(dim=1, keepdim = True)\n",
    "    tags_idx = (labels != pad_idx).nonzero()\n",
    "    correct = max_pred_idx[tags_idx].squeeze(1).eq(labels[tags_idx])\n",
    "    return correct.sum() / torch.FloatTensor([labels[tags_idx].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "attractive-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    for _,batch in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #model.zero_grad()\n",
    "                \n",
    "        input_ids = batch['ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "                \n",
    "        outputs = model(input_ids)\n",
    "                \n",
    "        # outputs = (max_len, batch_size, output_dim)\n",
    "        # labels = (batch_size, max_len)\n",
    "                \n",
    "        labels = labels.permute(1,0)\n",
    "        # labels = (max_len, batch_size)\n",
    "                \n",
    "        outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
    "                \n",
    "        labels = labels.contiguous().view(-1)\n",
    "                \n",
    "        # outputs = (max_len*batch_size, output_dim)\n",
    "        # labels = (max_len*batch_size)\n",
    "                \n",
    "        # Note: Seperating words into sentences is not necessary anymore, \n",
    "        #       we only care if an output word matches its label\n",
    "                                \n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = compute_accuracy(outputs, labels, tag_pad_idx)\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(training_loader), epoch_acc / len(training_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9860fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testing_loader, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _,batch in enumerate(testing_loader):\n",
    "            input_ids = batch['ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            # outputs = (max_len, batch_size, output_dim)\n",
    "            # labels = (batch_size, max_len)\n",
    "\n",
    "            labels = labels.permute(1,0)\n",
    "            # labels = (max_len, batch_size)\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
    "\n",
    "            labels = labels.contiguous().view(-1)\n",
    "\n",
    "            # outputs = (max_len*batch_size, output_dim)\n",
    "            # labels = (max_len*batch_size)\n",
    "\n",
    "            # Note: Seperating words into sentences is not necessary anymore, \n",
    "            #       we only care if an output word matches its label\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = compute_accuracy(outputs, labels, 0)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(testing_loader), epoch_acc / len(testing_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "written-hawaiian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "N_EPOCHS = 40\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6695201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Epoch: 01 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 2.810 | Train Acc: 25.18%\n",
      "\t Val. Loss: 2.547 |  Val. Acc: 29.38%\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Epoch: 02 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 2.292 | Train Acc: 33.93%\n",
      "\t Val. Loss: 2.203 |  Val. Acc: 41.50%\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Epoch: 03 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 2.080 | Train Acc: 41.44%\n",
      "\t Val. Loss: 2.021 |  Val. Acc: 47.44%\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Epoch: 04 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.924 | Train Acc: 45.86%\n",
      "\t Val. Loss: 1.869 |  Val. Acc: 50.85%\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Epoch: 05 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.786 | Train Acc: 49.21%\n",
      "\t Val. Loss: 1.729 |  Val. Acc: 53.57%\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "Epoch: 06 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.658 | Train Acc: 52.25%\n",
      "\t Val. Loss: 1.596 |  Val. Acc: 56.98%\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "Epoch: 07 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.534 | Train Acc: 55.52%\n",
      "\t Val. Loss: 1.470 |  Val. Acc: 59.49%\n",
      "\n",
      "======== Epoch 8 / 20 ========\n",
      "Epoch: 08 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.429 | Train Acc: 58.40%\n",
      "\t Val. Loss: 1.355 |  Val. Acc: 62.20%\n",
      "\n",
      "======== Epoch 9 / 20 ========\n",
      "Epoch: 09 | Epoch Time: 0m 31s\n",
      "\tTrain Loss: 1.324 | Train Acc: 61.30%\n",
      "\t Val. Loss: 1.252 |  Val. Acc: 65.10%\n",
      "\n",
      "======== Epoch 10 / 20 ========\n",
      "Epoch: 10 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.235 | Train Acc: 64.04%\n",
      "\t Val. Loss: 1.160 |  Val. Acc: 68.44%\n",
      "\n",
      "======== Epoch 11 / 20 ========\n",
      "Epoch: 11 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.159 | Train Acc: 66.76%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 71.00%\n",
      "\n",
      "======== Epoch 12 / 20 ========\n",
      "Epoch: 12 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.094 | Train Acc: 69.05%\n",
      "\t Val. Loss: 1.005 |  Val. Acc: 73.23%\n",
      "\n",
      "======== Epoch 13 / 20 ========\n",
      "Epoch: 13 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 1.033 | Train Acc: 70.81%\n",
      "\t Val. Loss: 0.944 |  Val. Acc: 75.18%\n",
      "\n",
      "======== Epoch 14 / 20 ========\n",
      "Epoch: 14 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.986 | Train Acc: 72.19%\n",
      "\t Val. Loss: 0.895 |  Val. Acc: 76.07%\n",
      "\n",
      "======== Epoch 15 / 20 ========\n",
      "Epoch: 15 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.950 | Train Acc: 73.01%\n",
      "\t Val. Loss: 0.852 |  Val. Acc: 77.18%\n",
      "\n",
      "======== Epoch 16 / 20 ========\n",
      "Epoch: 16 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.918 | Train Acc: 73.80%\n",
      "\t Val. Loss: 0.814 |  Val. Acc: 77.63%\n",
      "\n",
      "======== Epoch 17 / 20 ========\n",
      "Epoch: 17 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.887 | Train Acc: 74.43%\n",
      "\t Val. Loss: 0.786 |  Val. Acc: 78.37%\n",
      "\n",
      "======== Epoch 18 / 20 ========\n",
      "Epoch: 18 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.868 | Train Acc: 74.74%\n",
      "\t Val. Loss: 0.760 |  Val. Acc: 79.02%\n",
      "\n",
      "======== Epoch 19 / 20 ========\n",
      "Epoch: 19 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.847 | Train Acc: 75.39%\n",
      "\t Val. Loss: 0.739 |  Val. Acc: 79.20%\n",
      "\n",
      "======== Epoch 20 / 20 ========\n",
      "Epoch: 20 | Epoch Time: 0m 30s\n",
      "\tTrain Loss: 0.829 | Train Acc: 75.79%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 79.95%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch+1, N_EPOCHS))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, training_loader, optimizer, criterion, PAD_IDX)\n",
    "    valid_loss, valid_acc = evaluate(model, testing_loader, criterion, PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'postag-model.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6960bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
